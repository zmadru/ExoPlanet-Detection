{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_directml \n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from lib.LCWavelet import *\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallueModel(nn.Module):\n",
    "    def __init__(self, global_size=2001, local_size=201, num_classes=2):\n",
    "        super(ShallueModel, self).__init__()\n",
    "        self.global_size = global_size\n",
    "        self.local_size = local_size\n",
    "        self.num_classes = num_classes\n",
    "        if num_classes == 1:\n",
    "            print(\"Binary classification, sigmoid activation will be used.\")\n",
    "        elif num_classes > 1:\n",
    "            print(\"Multi-class classification, softmax activation will be used.\")\n",
    "            \n",
    "        \n",
    "        self.conv_global_odd = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "            nn.Conv1d(16, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "            nn.Conv1d(32, 64, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "            nn.Conv1d(64, 128, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "            nn.Conv1d(128, 256, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, 256, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.conv_global_even = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "            nn.Conv1d(16, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "            nn.Conv1d(32, 64, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "            nn.Conv1d(64, 128, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "            nn.Conv1d(128, 256, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, 256, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.conv_local_odd = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=7, stride=2),\n",
    "            nn.Conv1d(16, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=7, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.conv_local_even = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=7, stride=2),\n",
    "            nn.Conv1d(16, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=7, stride=2),\n",
    "        )\n",
    "        \n",
    "        # Calcular automáticamente el número de features resultantes de la concatenación\n",
    "        with torch.no_grad():\n",
    "            dummy_global = torch.zeros(1, 1, self.global_size)\n",
    "            dummy_local = torch.zeros(1, 1, self.local_size)\n",
    "            out_global_odd  = self.conv_global_odd(dummy_global)\n",
    "            out_global_even = self.conv_global_even(dummy_global)\n",
    "            out_local_odd   = self.conv_local_odd(dummy_local)\n",
    "            out_local_even  = self.conv_local_even(dummy_local)\n",
    "            \n",
    "            # Flatten cada salida y sumar sus dimensiones\n",
    "            num_features = out_global_odd.view(1, -1).size(1) + \\\n",
    "                           out_global_even.view(1, -1).size(1) + \\\n",
    "                           out_local_odd.view(1, -1).size(1) + \\\n",
    "                           out_local_even.view(1, -1).size(1)\n",
    "            \n",
    "        print(\"Número de features concatenados:\", num_features)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        global_odd = self.conv_global_odd(inputs[0])\n",
    "        global_even = self.conv_global_even(inputs[1])\n",
    "        local_odd = self.conv_local_odd(inputs[2])\n",
    "        local_even = self.conv_local_even(inputs[3])\n",
    "        \n",
    "        global_odd = global_odd.view(global_odd.size(0), -1)\n",
    "        global_even = global_even.view(global_even.size(0), -1)\n",
    "        local_odd = local_odd.view(local_odd.size(0), -1)\n",
    "        local_even = local_even.view(local_even.size(0), -1)\n",
    "        \n",
    "        # Concatenación de todas las ramas\n",
    "        x = torch.cat((global_odd, global_even, local_odd, local_even), dim=1)\n",
    "        x = self.fc(x)\n",
    "        if self.num_classes > 1:\n",
    "            return F.softmax(x, dim=1)\n",
    "        else:\n",
    "            return torch.sigmoid(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargado de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 9346/9346 [00:47<00:00, 194.97it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_data(path, file_name):\n",
    "    lc = LightCurveWaveletGlobalLocalCollection.from_pickle(path + file_name)\n",
    "    try:\n",
    "        getattr(lc, 'levels')\n",
    "    except AttributeError:\n",
    "        lc.levels = [1, 2, 3, 4]\n",
    "    return lc\n",
    "        \n",
    "path='all_data/'\n",
    "files = os.listdir(path)\n",
    "kepler_files = [f for f in files if f.endswith('.pickle')]\n",
    "light_curves = []\n",
    "\n",
    "for file in tqdm(kepler_files, desc='Loading data'):\n",
    "    light_curves.append(load_data(path, file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separar entre los confirmados y candidatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de candidatos: 2046\n",
      "Número de confirmados: 7300\n",
      "Clases: {'FALSE POSITIVE': 0, 'CONFIRMED': 1}\n"
     ]
    }
   ],
   "source": [
    "candidates = [lc for lc in light_curves if lc.headers['class'] == 'CANDIDATE']\n",
    "print(\"Número de candidatos:\", len(candidates))\n",
    "\n",
    "confirmed = [lc for lc in light_curves if lc.headers['class'] == 'CONFIRMED' or lc.headers['class'] == 'FALSE POSITIVE']\n",
    "print(\"Número de confirmados:\", len(confirmed))\n",
    "\n",
    "classes = [lc.headers['class'] for lc in confirmed]\n",
    "classes = set(classes)\n",
    "classes = {v: k for k, v in enumerate(classes)}\n",
    "print(\"Clases:\", classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing light curves: 100%|██████████| 7300/7300 [00:00<00:00, 88008.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de datos: 7300\n",
      "Elementos de cada clase: {0: 4637, 1: 2663}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "global_odd = []\n",
    "global_even = []\n",
    "local_odd = []\n",
    "local_even = []\n",
    "labels = []\n",
    "\n",
    "for lc in tqdm(confirmed, desc='Processing light curves'):\n",
    "    global_odd.append(lc.pliegue_impar_global._light_curve.flux.value)\n",
    "    global_even.append(lc.pliegue_par_global._light_curve.flux.value)\n",
    "    local_odd.append(lc.pliegue_impar_local._light_curve.flux.value)\n",
    "    local_even.append(lc.pliegue_par_local._light_curve.flux.value)\n",
    "    # Convertir la clase a un número entero\n",
    "    labels.append(classes[lc.headers['class']])\n",
    "    \n",
    "print(\"Número de datos:\", len(global_odd))\n",
    "print('Elementos de cada clase:', {k: labels.count(k) for k in set(labels)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separar las muestras en train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating items: 100%|██████████| 7300/7300 [00:00<00:00, 1611156.56it/s]\n",
      "e:\\Diego\\Astrofisica\\TFM\\ExoPlanet-Detection\\.venv\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento: 5110\n",
      "Tamaño del conjunto de prueba: 2190\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in tqdm(range(len(global_odd)), desc='Creating items'):\n",
    "    item = {\n",
    "        'global_odd': global_odd[i],\n",
    "        'global_even': global_even[i],\n",
    "        'local_odd': local_odd[i],\n",
    "        'local_even': local_even[i],\n",
    "        'label': labels[i]\n",
    "    }\n",
    "    items.append(item)\n",
    "\n",
    "train, test = train_test_split(items, test_size=0.3, random_state=42)\n",
    "train_global_odd = torch.tensor([item['global_odd'] for item in train])\n",
    "train_global_even = torch.tensor([item['global_even'] for item in train])\n",
    "train_local_odd = torch.tensor([item['local_odd'] for item in train])\n",
    "train_local_even = torch.tensor([item['local_even'] for item in train])\n",
    "train_labels = torch.tensor([item['label'] for item in train])\n",
    "\n",
    "test_global_odd = torch.tensor([item['global_odd'] for item in test])\n",
    "test_global_even = torch.tensor([item['global_even'] for item in test])\n",
    "test_local_odd = torch.tensor([item['local_odd'] for item in test])\n",
    "test_local_even = torch.tensor([item['local_even'] for item in test])\n",
    "test_labels = torch.tensor([item['label'] for item in test])\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(train_global_odd, train_global_even, train_local_odd, train_local_even, train_labels)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_global_odd, test_global_even, test_local_odd, test_local_even, test_labels)\n",
    "\n",
    "print(\"Tamaño del conjunto de entrenamiento:\", len(train_dataset))\n",
    "print(\"Tamaño del conjunto de prueba:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo: privateuseone:0\n"
     ]
    }
   ],
   "source": [
    "if torch_directml.is_available():\n",
    "    device = torch_directml.device()\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Dispositivo:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-class classification, softmax activation will be used.\n",
      "Número de features concatenados: 28672\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "learning_rate = 0.0001\n",
    "n_classes = 2 # si pones 1 usa sigmoid, si pones >1 usa softmax\n",
    "if n_classes > 1:\n",
    "    loss_fn = nn.CrossEntropyLoss() # para clasificación multiclase\n",
    "else:\n",
    "    loss_fn = nn.BCELoss() # para clasificación binaria con sigmoid\n",
    "    \n",
    "model = ShallueModel(global_size=2001, local_size=201, num_classes=n_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, train_loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    train_size = len(train_loader.dataset)\n",
    "    n_batches = len(train_loader)\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    for batch, data in enumerate(tqdm(train_loader, desc='Training')):\n",
    "        global_odd, global_even, local_odd, local_even, labels = data\n",
    "        # check if any tensor is empty\n",
    "        if global_odd.numel() == 0 or global_even.numel() == 0 or local_odd.numel() == 0 or local_even.numel() == 0:\n",
    "            continue\n",
    "        # check if any tensor has nan\n",
    "        if torch.isnan(global_odd).any() or torch.isnan(global_even).any() or torch.isnan(local_odd).any() or torch.isnan(local_even).any():\n",
    "            continue\n",
    "        # Move data to device\n",
    "        global_odd = global_odd.to(device).unsqueeze(1).float()\n",
    "        global_even = global_even.to(device).unsqueeze(1).float()\n",
    "        local_odd = local_odd.to(device).unsqueeze(1).float()\n",
    "        local_even = local_even.to(device).unsqueeze(1).float()\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # Forward propagation\n",
    "        outputs = model((global_odd, global_even, local_odd, local_even))\n",
    "        \n",
    "        if type(loss_fn) == nn.BCELoss:\n",
    "            # elimnar la dimensión extra de outputs\n",
    "            outputs = outputs.squeeze(1)\n",
    "            labels = labels.float()\n",
    "        # Compute loss and backpropagation\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        if type(loss_fn) == nn.BCELoss:\n",
    "            predicted = (outputs > 0.5).float()\n",
    "        else:\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / train_size\n",
    "    train_loss = total_loss / n_batches\n",
    "        \n",
    "    return train_loss, accuracy\n",
    "\n",
    "\n",
    "def val_fn(model, test_loader, loss_fn):\n",
    "    model.eval()\n",
    "    test_size = len(test_loader.dataset)\n",
    "    n_batches = len(test_loader)\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch, data in enumerate(tqdm(test_loader, desc='Validation')):\n",
    "            global_odd, global_even, local_odd, local_even, labels = data\n",
    "            # check if any tensor is empty\n",
    "            if global_odd.numel() == 0 or global_even.numel() == 0 or local_odd.numel() == 0 or local_even.numel() == 0:\n",
    "                continue\n",
    "            # check if any tensor has nan\n",
    "            if torch.isnan(global_odd).any() or torch.isnan(global_even).any() or torch.isnan(local_odd).any() or torch.isnan(local_even).any():\n",
    "                continue\n",
    "            \n",
    "            # Move data to device\n",
    "            global_odd = global_odd.to(device).unsqueeze(1).float()\n",
    "            global_even = global_even.to(device).unsqueeze(1).float()\n",
    "            local_odd = local_odd.to(device).unsqueeze(1).float()\n",
    "            local_even = local_even.to(device).unsqueeze(1).float()\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model((global_odd, global_even, local_odd, local_even))\n",
    "            if type(loss_fn) == nn.BCELoss:\n",
    "            # elimnar la dimensión extra de outputs\n",
    "                outputs = outputs.squeeze(1)\n",
    "                labels = labels.float()\n",
    "                \n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if type(loss_fn) == nn.BCELoss:\n",
    "                predicted = (outputs > 0.5).float()\n",
    "            else:\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            labels = labels.cpu().numpy()\n",
    "            predicted = predicted.cpu().numpy()\n",
    "            \n",
    "            all_labels.extend(labels)\n",
    "            all_predictions.extend(predicted)\n",
    "    \n",
    "    accuracy = correct / test_size\n",
    "    val_loss = total_loss / n_batches\n",
    "    \n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "    recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "    \n",
    "    return val_loss, accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 ------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:03<00:00, 22.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6238, Train Accuracy: 0.6051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 35/35 [00:00<00:00, 104.38it/s]\n",
      "e:\\Diego\\Astrofisica\\TFM\\ExoPlanet-Detection\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6118, Validation Accuracy: 0.5945\n",
      "F1 Score: 0.4888, Precision: 0.3987, Recall: 0.6314\n",
      "Epoch 2/10 ------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:03<00:00, 23.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5980, Train Accuracy: 0.6157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 35/35 [00:00<00:00, 100.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5754, Validation Accuracy: 0.6566\n",
      "F1 Score: 0.6738, Precision: 0.6888, Recall: 0.6974\n",
      "Epoch 3/10 ------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:03<00:00, 23.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5732, Train Accuracy: 0.6734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 35/35 [00:00<00:00, 104.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5557, Validation Accuracy: 0.6708\n",
      "F1 Score: 0.7071, Precision: 0.7059, Recall: 0.7124\n",
      "Epoch 4/10 ------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:03<00:00, 23.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5560, Train Accuracy: 0.6775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 35/35 [00:00<00:00, 103.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5502, Validation Accuracy: 0.6726\n",
      "F1 Score: 0.7049, Precision: 0.7063, Recall: 0.7144\n",
      "Epoch 5/10 ------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:03<00:00, 23.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5552, Train Accuracy: 0.6773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 35/35 [00:00<00:00, 104.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5500, Validation Accuracy: 0.6749\n",
      "F1 Score: 0.7002, Precision: 0.7101, Recall: 0.7168\n",
      "Epoch 6/10 ------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:03<00:00, 23.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5422, Train Accuracy: 0.6998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 35/35 [00:00<00:00, 103.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5377, Validation Accuracy: 0.6790\n",
      "F1 Score: 0.7208, Precision: 0.7205, Recall: 0.7211\n",
      "Epoch 7/10 ------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:03<00:00, 23.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5213, Train Accuracy: 0.7147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 35/35 [00:00<00:00, 97.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5205, Validation Accuracy: 0.7082\n",
      "F1 Score: 0.7497, Precision: 0.7487, Recall: 0.7522\n",
      "Epoch 8/10 ------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:03<00:00, 23.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5125, Train Accuracy: 0.7266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 35/35 [00:00<00:00, 98.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5144, Validation Accuracy: 0.7119\n",
      "F1 Score: 0.7481, Precision: 0.7517, Recall: 0.7561\n",
      "Epoch 9/10 ------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:03<00:00, 23.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5155, Train Accuracy: 0.7260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 35/35 [00:00<00:00, 100.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5420, Validation Accuracy: 0.6836\n",
      "F1 Score: 0.6928, Precision: 0.7406, Recall: 0.7260\n",
      "Epoch 10/10 ------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:03<00:00, 23.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5197, Train Accuracy: 0.7364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 35/35 [00:00<00:00, 100.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5091, Validation Accuracy: 0.7219\n",
      "F1 Score: 0.7629, Precision: 0.7628, Recall: 0.7667\n",
      "Entrenamiento y validación completados. ------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}', '------'*20)\n",
    "    train_loss, train_accuracy = train_fn(model, train_loader, optimizer, loss_fn)\n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "    \n",
    "    val_loss, val_accuracy, f1, precision, recall = val_fn(model, test_loader, loss_fn)\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}')\n",
    "\n",
    "print(\"Entrenamiento y validación completados.\", '------'*20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
