{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from lib.LCWavelet import *\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallueModel(nn.Module):\n",
    "    def __init__(self, global_size=2001, local_size=201, num_classes=2):\n",
    "        super(ShallueModel, self).__init__()\n",
    "        self.global_size = global_size\n",
    "        self.local_size = local_size\n",
    "        \n",
    "        self.conv_global_odd = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "            nn.Conv1d(16, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "            nn.Conv1d(32, 64, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "            nn.Conv1d(64, 128, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "            nn.Conv1d(128, 256, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, 256, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.conv_global_even = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "            nn.Conv1d(16, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "            nn.Conv1d(32, 64, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "            nn.Conv1d(64, 128, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "            nn.Conv1d(128, 256, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, 256, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.conv_local_odd = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=7, stride=2),\n",
    "            nn.Conv1d(16, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=7, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.conv_local_even = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=7, stride=2),\n",
    "            nn.Conv1d(16, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=7, stride=2),\n",
    "        )\n",
    "        \n",
    "        # Calcular automáticamente el número de features resultantes de la concatenación\n",
    "        with torch.no_grad():\n",
    "            dummy_global = torch.zeros(1, 1, self.global_size)\n",
    "            dummy_local = torch.zeros(1, 1, self.local_size)\n",
    "            out_global_odd  = self.conv_global_odd(dummy_global)\n",
    "            out_global_even = self.conv_global_even(dummy_global)\n",
    "            out_local_odd   = self.conv_local_odd(dummy_local)\n",
    "            out_local_even  = self.conv_local_even(dummy_local)\n",
    "            \n",
    "            # Flatten cada salida y sumar sus dimensiones\n",
    "            num_features = out_global_odd.view(1, -1).size(1) + \\\n",
    "                           out_global_even.view(1, -1).size(1) + \\\n",
    "                           out_local_odd.view(1, -1).size(1) + \\\n",
    "                           out_local_even.view(1, -1).size(1)\n",
    "            \n",
    "        print(\"Número de features concatenados:\", num_features)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        global_odd = self.conv_global_odd(inputs[0])\n",
    "        global_even = self.conv_global_even(inputs[1])\n",
    "        local_odd = self.conv_local_odd(inputs[2])\n",
    "        local_even = self.conv_local_even(inputs[3])\n",
    "        \n",
    "        global_odd = global_odd.view(global_odd.size(0), -1)\n",
    "        global_even = global_even.view(global_even.size(0), -1)\n",
    "        local_odd = local_odd.view(local_odd.size(0), -1)\n",
    "        local_even = local_even.view(local_even.size(0), -1)\n",
    "        \n",
    "        # Concatenación de todas las ramas\n",
    "        x = torch.cat((global_odd, global_even, local_odd, local_even), dim=1)\n",
    "        x = self.fc(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargado de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 9346/9346 [00:44<00:00, 208.12it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_data(path, file_name):\n",
    "    lc = LightCurveWaveletGlobalLocalCollection.from_pickle(path + file_name)\n",
    "    try:\n",
    "        getattr(lc, 'levels')\n",
    "    except AttributeError:\n",
    "        lc.levels = [1, 2, 3, 4]\n",
    "    return lc\n",
    "        \n",
    "path='all_data/'\n",
    "files = os.listdir(path)\n",
    "kepler_files = [f for f in files if f.endswith('.pickle')]\n",
    "light_curves = []\n",
    "\n",
    "for file in tqdm(kepler_files, desc='Loading data'):\n",
    "    light_curves.append(load_data(path, file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separar entre los confirmados y candidatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de candidatos: 2046\n",
      "Número de confirmados: 7300\n",
      "Clases: {'FALSE POSITIVE': 0, 'CONFIRMED': 1}\n"
     ]
    }
   ],
   "source": [
    "candidates = [lc for lc in light_curves if lc.headers['class'] == 'CANDIDATE']\n",
    "print(\"Número de candidatos:\", len(candidates))\n",
    "\n",
    "confirmed = [lc for lc in light_curves if lc.headers['class'] == 'CONFIRMED' or lc.headers['class'] == 'FALSE POSITIVE']\n",
    "print(\"Número de confirmados:\", len(confirmed))\n",
    "\n",
    "classes = [lc.headers['class'] for lc in confirmed]\n",
    "classes = set(classes)\n",
    "classes = {v: k for k, v in enumerate(classes)}\n",
    "print(\"Clases:\", classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing light curves: 100%|██████████| 7300/7300 [00:00<00:00, 171024.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de datos: 7300\n",
      "Elementos de cada clase: {0: 4637, 1: 2663}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "global_odd = []\n",
    "global_even = []\n",
    "local_odd = []\n",
    "local_even = []\n",
    "labels = []\n",
    "\n",
    "for lc in tqdm(confirmed, desc='Processing light curves'):\n",
    "    global_odd.append(lc.pliegue_impar_global._light_curve.flux.value)\n",
    "    global_even.append(lc.pliegue_par_global._light_curve.flux.value)\n",
    "    local_odd.append(lc.pliegue_impar_local._light_curve.flux.value)\n",
    "    local_even.append(lc.pliegue_par_local._light_curve.flux.value)\n",
    "    # Convertir la clase a un número entero\n",
    "    labels.append(classes[lc.headers['class']])\n",
    "    \n",
    "print(\"Número de datos:\", len(global_odd))\n",
    "print('Elementos de cada clase:', {k: labels.count(k) for k in set(labels)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separar las muestras en train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating items: 100%|██████████| 7300/7300 [00:00<00:00, 1823067.53it/s]\n",
      "e:\\Diego\\Astrofisica\\TFM\\ExoPlanet-Detection\\.venv\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento: 5110\n",
      "Tamaño del conjunto de prueba: 2190\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in tqdm(range(len(global_odd)), desc='Creating items'):\n",
    "    item = {\n",
    "        'global_odd': global_odd[i],\n",
    "        'global_even': global_even[i],\n",
    "        'local_odd': local_odd[i],\n",
    "        'local_even': local_even[i],\n",
    "        'label': labels[i]\n",
    "    }\n",
    "    items.append(item)\n",
    "\n",
    "train, test = train_test_split(items, test_size=0.3, random_state=42)\n",
    "train_local_odd = torch.tensor([item['local_odd'] for item in train])\n",
    "train_local_even = torch.tensor([item['local_even'] for item in train])\n",
    "train_global_odd = torch.tensor([item['global_odd'] for item in train])\n",
    "train_global_even = torch.tensor([item['global_even'] for item in train])\n",
    "train_labels = torch.tensor([item['label'] for item in train])\n",
    "\n",
    "test_local_odd = torch.tensor([item['local_odd'] for item in test])\n",
    "test_local_even = torch.tensor([item['local_even'] for item in test])\n",
    "test_global_odd = torch.tensor([item['global_odd'] for item in test])\n",
    "test_global_even = torch.tensor([item['global_even'] for item in test])\n",
    "test_labels = torch.tensor([item['label'] for item in test])\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(train_global_odd, train_global_even, train_local_odd, train_local_even, train_labels)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_global_odd, test_global_even, test_local_odd, test_local_even, test_labels)\n",
    "\n",
    "print(\"Tamaño del conjunto de entrenamiento:\", len(train_dataset))\n",
    "print(\"Tamaño del conjunto de prueba:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de features concatenados: 28672\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model = ShallueModel(global_size=2001, local_size=201, num_classes=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, train_loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    train_size = len(train_loader.dataset)\n",
    "    n_batches = len(train_loader)\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    for batch, data in enumerate(tqdm(train_loader, desc='Training')):\n",
    "        global_odd, global_even, local_odd, local_even, labels = data\n",
    "        # check if any tensor is empty\n",
    "        if global_odd.numel() == 0 or global_even.numel() == 0 or local_odd.numel() == 0 or local_even.numel() == 0:\n",
    "            continue\n",
    "        # check if any tensor has nan\n",
    "        if torch.isnan(global_odd).any() or torch.isnan(global_even).any() or torch.isnan(local_odd).any() or torch.isnan(local_even).any():\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # Forward propagation\n",
    "        outputs = model((global_odd.unsqueeze(1).float(), global_even.unsqueeze(1).float(), local_odd.unsqueeze(1).float(), local_even.unsqueeze(1).float()))\n",
    "        \n",
    "        # Compute loss and backpropagation\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / train_size\n",
    "    train_loss = total_loss / n_batches\n",
    "        \n",
    "    return train_loss, accuracy\n",
    "\n",
    "\n",
    "def val_fn(model, test_loader, loss_fn):\n",
    "    model.eval()\n",
    "    test_size = len(test_loader.dataset)\n",
    "    n_batches = len(test_loader)\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch, data in enumerate(tqdm(test_loader, desc='Validation')):\n",
    "            global_odd, global_even, local_odd, local_even, labels = data\n",
    "            # check if any tensor is empty\n",
    "            if global_odd.numel() == 0 or global_even.numel() == 0 or local_odd.numel() == 0 or local_even.numel() == 0:\n",
    "                continue\n",
    "            # check if any tensor has nan\n",
    "            if torch.isnan(global_odd).any() or torch.isnan(global_even).any() or torch.isnan(local_odd).any() or torch.isnan(local_even).any():\n",
    "                continue\n",
    "            \n",
    "            outputs = model((global_odd.unsqueeze(1).float(), global_even.unsqueeze(1).float(), local_odd.unsqueeze(1).float(), local_even.unsqueeze(1).float()))\n",
    "            \n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_predictions.extend(predicted.numpy())\n",
    "    \n",
    "    accuracy = correct / test_size\n",
    "    val_loss = total_loss / n_batches\n",
    "    \n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "    recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "    \n",
    "    return val_loss, accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 ------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:14<00:00,  5.48it/s]\n",
      "Validation: 100%|██████████| 35/35 [00:01<00:00, 26.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5935, Train Accuracy: 0.6317\n",
      "Validation Loss: 0.5614, Validation Accuracy: 0.6534\n",
      "F1 Score: 0.6929, Precision: 0.6920, Recall: 0.6940\n",
      "Epoch 2/10 ------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:16<00:00,  4.93it/s]\n",
      "Validation: 100%|██████████| 35/35 [00:01<00:00, 26.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5543, Train Accuracy: 0.6765\n",
      "Validation Loss: 0.5426, Validation Accuracy: 0.6703\n",
      "F1 Score: 0.7075, Precision: 0.7061, Recall: 0.7119\n",
      "Epoch 3/10 ------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:17<00:00,  4.58it/s]\n",
      "Validation: 100%|██████████| 35/35 [00:01<00:00, 25.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5435, Train Accuracy: 0.6926\n",
      "Validation Loss: 0.5383, Validation Accuracy: 0.6836\n",
      "F1 Score: 0.7219, Precision: 0.7208, Recall: 0.7260\n",
      "Epoch 4/10 ------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:20<00:00,  3.97it/s]\n",
      "Validation: 100%|██████████| 35/35 [00:01<00:00, 26.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5317, Train Accuracy: 0.7020\n",
      "Validation Loss: 0.5308, Validation Accuracy: 0.6904\n",
      "F1 Score: 0.7268, Precision: 0.7271, Recall: 0.7333\n",
      "Epoch 5/10 ------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:22<00:00,  3.55it/s]\n",
      "Validation: 100%|██████████| 35/35 [00:01<00:00, 26.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5274, Train Accuracy: 0.7090\n",
      "Validation Loss: 0.5302, Validation Accuracy: 0.6886\n",
      "F1 Score: 0.7176, Precision: 0.7261, Recall: 0.7313\n",
      "Epoch 6/10 ------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:22<00:00,  3.55it/s]\n",
      "Validation: 100%|██████████| 35/35 [00:01<00:00, 26.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5194, Train Accuracy: 0.7155\n",
      "Validation Loss: 0.5233, Validation Accuracy: 0.7005\n",
      "F1 Score: 0.7362, Precision: 0.7384, Recall: 0.7439\n",
      "Epoch 7/10 ------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:31<00:00,  2.56it/s]\n",
      "Validation: 100%|██████████| 35/35 [00:01<00:00, 26.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5154, Train Accuracy: 0.7235\n",
      "Validation Loss: 0.5273, Validation Accuracy: 0.6991\n",
      "F1 Score: 0.7311, Precision: 0.7379, Recall: 0.7425\n",
      "Epoch 8/10 ------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:26<00:00,  3.05it/s]\n",
      "Validation: 100%|██████████| 35/35 [00:01<00:00, 26.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5106, Train Accuracy: 0.7284\n",
      "Validation Loss: 0.5208, Validation Accuracy: 0.7027\n",
      "F1 Score: 0.7371, Precision: 0.7414, Recall: 0.7464\n",
      "Epoch 9/10 ------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:29<00:00,  2.74it/s]\n",
      "Validation: 100%|██████████| 35/35 [00:01<00:00, 26.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5120, Train Accuracy: 0.7427\n",
      "Validation Loss: 0.5070, Validation Accuracy: 0.7196\n",
      "F1 Score: 0.7632, Precision: 0.7625, Recall: 0.7643\n",
      "Epoch 10/10 ------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:25<00:00,  3.15it/s]\n",
      "Validation: 100%|██████████| 35/35 [00:01<00:00, 26.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4947, Train Accuracy: 0.7442\n",
      "Validation Loss: 0.5158, Validation Accuracy: 0.7078\n",
      "F1 Score: 0.7499, Precision: 0.7489, Recall: 0.7517\n",
      "Entrenamiento y validación completados.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}', '------'*20)\n",
    "    train_loss, train_accuracy = train_fn(model, train_loader, optimizer, loss_fn)\n",
    "    val_loss, val_accuracy, f1, precision, recall = val_fn(model, test_loader, loss_fn)\n",
    "    \n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}')\n",
    "print(\"Entrenamiento y validación completados.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
