{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torch_directml \n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from lib.LCWavelet import *\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallueModel(nn.Module):\n",
    "    def __init__(self, global_size=2001, local_size=201, num_classes=2):\n",
    "        super(ShallueModel, self).__init__()\n",
    "        self.global_size = global_size\n",
    "        self.local_size = local_size\n",
    "        self.num_classes = num_classes\n",
    "        if num_classes == 1:\n",
    "            print(\"Binary classification, sigmoid activation will be used.\")\n",
    "        elif num_classes > 1:\n",
    "            print(\"Multi-class classification, softmax activation will be used.\")\n",
    "            \n",
    "        \n",
    "        self.conv_global_odd = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "            nn.Conv1d(16, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "            nn.Conv1d(32, 64, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "            nn.Conv1d(64, 128, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "            nn.Conv1d(128, 256, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, 256, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.conv_global_even = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "            nn.Conv1d(16, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "            nn.Conv1d(32, 64, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "            nn.Conv1d(64, 128, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "            nn.Conv1d(128, 256, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, 256, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.conv_local_odd = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=7, stride=2),\n",
    "            nn.Conv1d(16, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=7, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.conv_local_even = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=7, stride=2),\n",
    "            nn.Conv1d(16, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=7, stride=2),\n",
    "        )\n",
    "        \n",
    "        # Calcular automáticamente el número de features resultantes de la concatenación\n",
    "        with torch.no_grad():\n",
    "            dummy_global = torch.zeros(1, 1, self.global_size)\n",
    "            dummy_local = torch.zeros(1, 1, self.local_size)\n",
    "            out_global_odd  = self.conv_global_odd(dummy_global)\n",
    "            out_global_even = self.conv_global_even(dummy_global)\n",
    "            out_local_odd   = self.conv_local_odd(dummy_local)\n",
    "            out_local_even  = self.conv_local_even(dummy_local)\n",
    "            \n",
    "            # Flatten cada salida y sumar sus dimensiones\n",
    "            num_features = out_global_odd.view(1, -1).size(1) + \\\n",
    "                           out_global_even.view(1, -1).size(1) + \\\n",
    "                           out_local_odd.view(1, -1).size(1) + \\\n",
    "                           out_local_even.view(1, -1).size(1)\n",
    "            \n",
    "        print(\"Número de features concatenados:\", num_features)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        global_odd = self.conv_global_odd(inputs[0])\n",
    "        global_even = self.conv_global_even(inputs[1])\n",
    "        local_odd = self.conv_local_odd(inputs[2])\n",
    "        local_even = self.conv_local_even(inputs[3])\n",
    "        \n",
    "        global_odd = global_odd.view(global_odd.size(0), -1)\n",
    "        global_even = global_even.view(global_even.size(0), -1)\n",
    "        local_odd = local_odd.view(local_odd.size(0), -1)\n",
    "        local_even = local_even.view(local_even.size(0), -1)\n",
    "        \n",
    "        # Concatenación de todas las ramas\n",
    "        x = torch.cat((global_odd, global_even, local_odd, local_even), dim=1)\n",
    "        x = self.fc(x)\n",
    "        if self.num_classes > 1:\n",
    "            return F.softmax(x, dim=1)\n",
    "        else:\n",
    "            return torch.sigmoid(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargado de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 18608/18608 [04:10<00:00, 74.25it/s] \n"
     ]
    }
   ],
   "source": [
    "def load_data(path, file_name):\n",
    "    lc = LightCurveWaveletGlobalLocalCollection.from_pickle(path + file_name)\n",
    "    try:\n",
    "        getattr(lc, 'levels')\n",
    "    except AttributeError:\n",
    "        lc.levels = [1, 2, 3, 4]\n",
    "    return lc\n",
    "        \n",
    "path='all_data/'\n",
    "files = os.listdir(path)\n",
    "kepler_files = [f for f in files if f.endswith('.pickle')]\n",
    "light_curves = []\n",
    "\n",
    "for file in tqdm(kepler_files, desc='Loading data'):\n",
    "    light_curves.append(load_data(path, file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separar entre los confirmados y candidatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de candidatos: 4092\n",
      "Número de confirmados: 14516\n",
      "Clases: {'CONFIRMED': 0, 'FALSE POSITIVE': 1}\n"
     ]
    }
   ],
   "source": [
    "candidates = [lc for lc in light_curves if lc.headers['class'] == 'CANDIDATE']\n",
    "print(\"Número de candidatos:\", len(candidates))\n",
    "\n",
    "confirmed = [lc for lc in light_curves if lc.headers['class'] == 'CONFIRMED' or lc.headers['class'] == 'FALSE POSITIVE']\n",
    "print(\"Número de confirmados:\", len(confirmed))\n",
    "\n",
    "classes = [lc.headers['class'] for lc in confirmed]\n",
    "classes = set(classes)\n",
    "classes = {v: k for k, v in enumerate(classes)}\n",
    "print(\"Clases:\", classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing light curves: 100%|██████████| 14516/14516 [00:00<00:00, 89136.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de datos: 14516\n",
      "Elementos de cada clase: {0: 5328, 1: 9188}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "global_odd = []\n",
    "global_even = []\n",
    "local_odd = []\n",
    "local_even = []\n",
    "labels = []\n",
    "\n",
    "for lc in tqdm(confirmed, desc='Processing light curves'):\n",
    "    global_odd.append(lc.pliegue_impar_global._light_curve.flux.value)\n",
    "    global_even.append(lc.pliegue_par_global._light_curve.flux.value)\n",
    "    local_odd.append(lc.pliegue_impar_local._light_curve.flux.value)\n",
    "    local_even.append(lc.pliegue_par_local._light_curve.flux.value)\n",
    "    # Convertir la clase a un número entero\n",
    "    labels.append(classes[lc.headers['class']])\n",
    "    \n",
    "print(\"Número de datos:\", len(global_odd))\n",
    "print('Elementos de cada clase:', {k: labels.count(k) for k in set(labels)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separar las muestras en train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating items: 100%|██████████| 14516/14516 [00:00<00:00, 729881.40it/s]\n",
      "/opt/miniconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_2a19nf9hj1/croot/pytorch_1675190251927/work/torch/csrc/utils/tensor_new.cpp:204.)\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento: 10161\n",
      "Tamaño del conjunto de prueba: 4355\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in tqdm(range(len(global_odd)), desc='Creating items'):\n",
    "    item = {\n",
    "        'global_odd': global_odd[i],\n",
    "        'global_even': global_even[i],\n",
    "        'local_odd': local_odd[i],\n",
    "        'local_even': local_even[i],\n",
    "        'label': labels[i]\n",
    "    }\n",
    "    items.append(item)\n",
    "\n",
    "train, test = train_test_split(items, test_size=0.3, random_state=42)\n",
    "train_global_odd = torch.tensor([item['global_odd'] for item in train])\n",
    "train_global_even = torch.tensor([item['global_even'] for item in train])\n",
    "train_local_odd = torch.tensor([item['local_odd'] for item in train])\n",
    "train_local_even = torch.tensor([item['local_even'] for item in train])\n",
    "train_labels = torch.tensor([item['label'] for item in train])\n",
    "\n",
    "test_global_odd = torch.tensor([item['global_odd'] for item in test])\n",
    "test_global_even = torch.tensor([item['global_even'] for item in test])\n",
    "test_local_odd = torch.tensor([item['local_odd'] for item in test])\n",
    "test_local_even = torch.tensor([item['local_even'] for item in test])\n",
    "test_labels = torch.tensor([item['label'] for item in test])\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(train_global_odd, train_global_even, train_local_odd, train_local_even, train_labels)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_global_odd, test_global_even, test_local_odd, test_local_even, test_labels)\n",
    "\n",
    "print(\"Tamaño del conjunto de entrenamiento:\", len(train_dataset))\n",
    "print(\"Tamaño del conjunto de prueba:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch_directml' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jc/pm1gyg0x00d3fn3t30rhls985kth5z/T/ipykernel_8478/3970979954.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mtorch_directml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_directml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dispositivo:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch_directml' is not defined"
     ]
    }
   ],
   "source": [
    "if torch_directml.is_available():\n",
    "    device = torch_directml.device()\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Dispositivo:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Dispositivo:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-class classification, softmax activation will be used.\n",
      "Número de features concatenados: 28672\n",
      "No se pudo cargar el modelo. Se inicializa uno nuevo.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "learning_rate = 0.0001\n",
    "n_classes = 2 # si pones 1 usa sigmoid, si pones >1 usa softmax\n",
    "if n_classes > 1:\n",
    "    loss_fn = nn.CrossEntropyLoss() # para clasificación multiclase\n",
    "else:\n",
    "    loss_fn = nn.BCELoss() # para clasificación binaria con sigmoid\n",
    "    \n",
    "model = ShallueModel(global_size=2001, local_size=201, num_classes=n_classes).to(device)\n",
    "\n",
    "try: \n",
    "    model.load_state_dict(torch.load('Shallue_model.pth'))\n",
    "    model.eval()\n",
    "except:\n",
    "    print(\"No se pudo cargar el modelo. Se inicializa uno nuevo.\")\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, train_loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    train_size = len(train_loader.dataset)\n",
    "    n_batches = len(train_loader)\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    for batch, data in enumerate(tqdm(train_loader, desc='Training')):\n",
    "        global_odd, global_even, local_odd, local_even, labels = data\n",
    "        # check if any tensor is empty\n",
    "        if global_odd.numel() == 0 or global_even.numel() == 0 or local_odd.numel() == 0 or local_even.numel() == 0:\n",
    "            continue\n",
    "        # check if any tensor has nan\n",
    "        if torch.isnan(global_odd).any() or torch.isnan(global_even).any() or torch.isnan(local_odd).any() or torch.isnan(local_even).any():\n",
    "            continue\n",
    "        # Move data to device\n",
    "        global_odd = global_odd.to(device).unsqueeze(1).float()\n",
    "        global_even = global_even.to(device).unsqueeze(1).float()\n",
    "        local_odd = local_odd.to(device).unsqueeze(1).float()\n",
    "        local_even = local_even.to(device).unsqueeze(1).float()\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # Forward propagation\n",
    "        outputs = model((global_odd, global_even, local_odd, local_even))\n",
    "        \n",
    "        if type(loss_fn) == nn.BCELoss:\n",
    "            # elimnar la dimensión extra de outputs\n",
    "            outputs = outputs.squeeze(1)\n",
    "            labels = labels.float()\n",
    "        # Compute loss and backpropagation\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        if type(loss_fn) == nn.BCELoss:\n",
    "            predicted = (outputs > 0.5).float()\n",
    "        else:\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / train_size\n",
    "    train_loss = total_loss / n_batches\n",
    "        \n",
    "    return train_loss, accuracy\n",
    "\n",
    "\n",
    "def val_fn(model, test_loader, loss_fn):\n",
    "    model.eval()\n",
    "    test_size = len(test_loader.dataset)\n",
    "    n_batches = len(test_loader)\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch, data in enumerate(tqdm(test_loader, desc='Validation')):\n",
    "            global_odd, global_even, local_odd, local_even, labels = data\n",
    "            # check if any tensor is empty\n",
    "            if global_odd.numel() == 0 or global_even.numel() == 0 or local_odd.numel() == 0 or local_even.numel() == 0:\n",
    "                continue\n",
    "            # check if any tensor has nan\n",
    "            if torch.isnan(global_odd).any() or torch.isnan(global_even).any() or torch.isnan(local_odd).any() or torch.isnan(local_even).any():\n",
    "                continue\n",
    "            \n",
    "            # Move data to device\n",
    "            global_odd = global_odd.to(device).unsqueeze(1).float()\n",
    "            global_even = global_even.to(device).unsqueeze(1).float()\n",
    "            local_odd = local_odd.to(device).unsqueeze(1).float()\n",
    "            local_even = local_even.to(device).unsqueeze(1).float()\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model((global_odd, global_even, local_odd, local_even))\n",
    "            if type(loss_fn) == nn.BCELoss:\n",
    "            # elimnar la dimensión extra de outputs\n",
    "                outputs = outputs.squeeze(1)\n",
    "                labels = labels.float()\n",
    "                \n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if type(loss_fn) == nn.BCELoss:\n",
    "                predicted = (outputs > 0.5).float()\n",
    "            else:\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            labels = labels.cpu().numpy()\n",
    "            predicted = predicted.cpu().numpy()\n",
    "            \n",
    "            all_labels.extend(labels)\n",
    "            all_predictions.extend(predicted)\n",
    "    \n",
    "    accuracy = correct / test_size\n",
    "    val_loss = total_loss / n_batches\n",
    "    \n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "    \n",
    "    return val_loss, accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 ------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 12/159 [00:18<03:15,  1.33s/it]"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}', '------'*20)\n",
    "    train_loss, train_accuracy = train_fn(model, train_loader, optimizer, loss_fn)\n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "    \n",
    "    val_loss, val_accuracy, f1, precision, recall = val_fn(model, test_loader, loss_fn)\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}')\n",
    "\n",
    "print(\"Entrenamiento y validación completados.\", '------'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'Shallue_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
